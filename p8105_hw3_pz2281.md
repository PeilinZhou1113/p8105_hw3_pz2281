p8105\_hw3\_pz2281
================
Peilin Zhou

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──

    ## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4
    ## ✓ tibble  3.1.4     ✓ dplyr   1.0.7
    ## ✓ tidyr   1.1.3     ✓ stringr 1.4.0
    ## ✓ readr   2.0.1     ✓ forcats 0.5.1

    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .8,
  out.width = "95%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

read in instacart data

``` r
library(p8105.datasets)
data("instacart")
instacart
```

    ## # A tibble: 1,384,617 × 15
    ##    order_id product_id add_to_cart_order reordered user_id eval_set order_number
    ##       <int>      <int>             <int>     <int>   <int> <chr>           <int>
    ##  1        1      49302                 1         1  112108 train               4
    ##  2        1      11109                 2         1  112108 train               4
    ##  3        1      10246                 3         0  112108 train               4
    ##  4        1      49683                 4         0  112108 train               4
    ##  5        1      43633                 5         1  112108 train               4
    ##  6        1      13176                 6         0  112108 train               4
    ##  7        1      47209                 7         0  112108 train               4
    ##  8        1      22035                 8         1  112108 train               4
    ##  9       36      39612                 1         0   79431 train              23
    ## 10       36      19660                 2         1   79431 train              23
    ## # … with 1,384,607 more rows, and 8 more variables: order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>, product_name <chr>,
    ## #   aisle_id <int>, department_id <int>, aisle <chr>, department <chr>

The instacart dataset contains 1384617 observations with 15 variables.
Each row represents a product from an order. The data contains customer
information with unique user\_id as a variable and there are 131209
unique customers. There are total of 21 departments, 134 aisles, and
39123 products in total. The products from each order\_id are ordered by
the add\_to\_cart\_order from the customer. The order\_hour\_of\_day
variable documents the hour of when the order was made. The average
order hour of a day for all customers in this dataset is 13.5775922.

``` r
aisle_freq = instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_obs = n()
  )
most_ord_aisle = aisle_freq %>% 
  slice_max(n_obs)
```

There are 134 aisles and the aisle that most items are ordered from is
fresh vegetables, 150609 items, which makes sense in real life, since
people often order vegetables online from instacart, amazon fresh etc.

``` r
aisle_freq %>% 
  filter(n_obs > 10000) %>% 
  arrange(n_obs) %>% 
  mutate(aisle = forcats::fct_inorder(aisle)) %>% 
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_col(alpha = 0.8) +
  labs(
    title = "Number of Items Ordered from Each Aisle",
    x = "Aisle Names",
    y = "Number of Items Ordered"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_pz2281_files/figure-gfm/unnamed-chunk-4-1.png" width="95%" />

From this barchart, we could see that fresh fruits and fresh vegetables
are the two aisles that the customers order most from. For each of these
two aisles, the number of ordered products are around 150000, which
exceed the number of ordered items from other aisles significantly. In
contrast, the least ordered aisle among the aisles with ordered items
over 10000 is butter.

``` r
bake_ingr_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "baking ingredients") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)
  
dog_food_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "dog food care") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)

pack_vegfru_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)
```

``` r
order_table = bind_rows(bake_ingr_order, dog_food_order, pack_vegfru_order) %>%   relocate(order_rank, .before = n)
order_table
```

    ## # A tibble: 9 × 4
    ## # Groups:   aisle [3]
    ##   aisle                      product_name                       order_rank     n
    ##   <chr>                      <chr>                                   <int> <int>
    ## 1 baking ingredients         Light Brown Sugar                           1   499
    ## 2 baking ingredients         Pure Baking Soda                            2   387
    ## 3 baking ingredients         Cane Sugar                                  3   336
    ## 4 dog food care              Snack Sticks Chicken & Rice Recip…          1    30
    ## 5 dog food care              Organix Chicken & Brown Rice Reci…          2    28
    ## 6 dog food care              Small Dog Biscuits                          3    26
    ## 7 packaged vegetables fruits Organic Baby Spinach                        1  9784
    ## 8 packaged vegetables fruits Organic Raspberries                         2  5546
    ## 9 packaged vegetables fruits Organic Blueberries                         3  4966

The top 3 products in the baking ingredients aisle are Light Brown
Sugar, Pure Baking Soda, and Cane Sugar. It makes sense since these
three items are ingredients generally necessary for making breads,
cakes, and other desserts. The top 3 products in the dog food care aisle
are Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken &
Brown Rice Recipe, and Small Dog Biscuits. This shows that people often
like to purchase snacks for their dogs online. The top 3 products in the
packaged vegetables fruits aisle are Organic Baby Spinach, Organic
Raspberries, and Organic Blueberries. It shows that people prefer
organic products for fruits and veggies when they purchase packaged
ones.

Now move to get tables for pink lady apple and coffee ice cream

``` r
apple_mean_hour = instacart %>% 
  filter(product_name == "Pink Lady Apples") %>% 
  group_by(order_dow) %>% 
  summarize(
    mean_hour_day = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_day
  ) %>% 
  mutate(item_name = "Pink Lady Apple") %>% 
  relocate(item_name)

cof_ice_mean_hour = instacart %>% 
  filter(product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow) %>% 
  summarize(
    mean_hour_day = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_day
  ) %>% 
  mutate(item_name = "Coffee Ice Cream") %>% 
  relocate(item_name)

table_mean_hour_day = bind_rows(apple_mean_hour, cof_ice_mean_hour) %>% 
  knitr::kable("simple")
table_mean_hour_day
```

| item\_name       |        0 |        1 |        2 |        3 |        4 |        5 |        6 |
|:-----------------|---------:|---------:|---------:|---------:|---------:|---------:|---------:|
| Pink Lady Apple  | 13.44118 | 11.36000 | 11.70213 | 14.25000 | 11.55172 | 12.78431 | 11.93750 |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 | 15.31818 | 15.21739 | 12.26316 | 13.83333 |

The table shows that customers generally order Pink Lady Apple around
noon, from 11am to 13pm. The hours that they order coffee ice cream are
generally later, from noon to the afternoon. The average hour of order
for each day ranges from 12pm to almost 14pm.

## Problem 2

load the brfss data first

``` r
data("brfss_smart2010")
```

``` r
brfss_health_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, location_state = locationdesc, resp_id = respid) %>% 
  mutate(resp_id = gsub("RESP", "", resp_id)) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
#since the responses for the "overall health" question were only in form of poor, fair, good, very good, to excellent, we don't need to do another filter operation.
```

Find states that were observed at 7 or more locations for 2002 and 2010.

``` r
brfss_health_clean = brfss_health_clean %>% 
  mutate(year = as.character(year))

brfss_health_clean %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  summarise(
    n_loc = n_distinct(location_state)
  ) %>% 
  filter(n_loc >= 7)
```

    ## # A tibble: 6 × 2
    ##   state n_loc
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 MA        8
    ## 4 NC        7
    ## 5 NJ        8
    ## 6 PA       10

There were 6 states observed in more than 7 locations for year 2002.
They were Connecticut, Florida, Massachusetts, North Carolina, New
Jersey, and Pennsylvania.

``` r
brfss_health_clean %>% 
  filter(year == "2010") %>% 
  group_by(state) %>% 
  summarise(
    n_loc = n_distinct(location_state)
  ) %>% 
  filter(n_loc >= 7)
```

    ## # A tibble: 14 × 2
    ##    state n_loc
    ##    <chr> <int>
    ##  1 CA       12
    ##  2 CO        7
    ##  3 FL       41
    ##  4 MA        9
    ##  5 MD       12
    ##  6 NC       12
    ##  7 NE       10
    ##  8 NJ       19
    ##  9 NY        9
    ## 10 OH        8
    ## 11 PA        7
    ## 12 SC        7
    ## 13 TX       16
    ## 14 WA       10

There were 14 states in 2010 which were observed in more than 7
locations. It was 8 states more than that in 2002. The difference shows
that more health data were collected in different locations for health
assessment.

``` r
brfss_health_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(
    mean_data_value = mean(data_value)
  ) %>% 
  ggplot(aes(x = year, y = mean_data_value)) +
  geom_line(aes(group = state, color = state)) +
  labs(
    title = "Spaghetti Plot for Average Data Value of Each State across Years"
  )
```

    ## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.

    ## Warning: Removed 3 row(s) containing missing values (geom_path).

<img src="p8105_hw3_pz2281_files/figure-gfm/unnamed-chunk-12-1.png" width="95%" />

The plot illustrates the average\_data\_value of the survey participants
whose answer were “excellent” to the question about the topic of overall
health for each state in a time range from 2002 to 2010. It is clear to
observe that in 2005, there is a state that has the lowest mean data
value, and the following second lowest were from another state in the
year of 2007, and the third lowest was in year 2009, from the same state
that has the lowest mean-data-value. The highest mean-data-value was
from the year of 2002.

Next, making plots for the distribution of data\_value in NY state from
2006 and 2010.

``` r
plot_2010 = brfss_health_clean %>% 
  filter(year == "2010" & state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, group = location_state, color = location_state)) +
  geom_line() +
  theme_minimal() + theme(legend.position  = "right") +
  labs(
    title = "Data Values Based on Response Level for Locations in NY state for 2010"
  )

plot_2006 = brfss_health_clean %>% 
  filter(year == "2006" & state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = location_state)) +
  geom_line(aes(group = location_state)) +
  labs(
    title = "Data Values Based on Response Level for Locations in NY state for 2006"
  ) +
  theme_minimal() +
  theme(legend.position  = "right")

plot_2006 / plot_2010
```

<img src="p8105_hw3_pz2281_files/figure-gfm/unnamed-chunk-13-1.png" width="95%" />

Based on the two-panel plot, we can see that the data value trend of
responses are roughly the same in the year of 2006 and 2010 in NY state.
Response as “Good” and “Vary good” have the largest data value in both
years. and the data value of response “poor” have the lowest value. The
biggest difference between the year of 2006 and 2010 is that, in 2010,
response “Very good” has the largest data value, while back in 2006,
response “Good” has the largest value.

\#\#Problem 3 load csv data

``` r
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(day_type = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>% 
  mutate(day_type = factor(day_type)) %>% 
  relocate(week, day_id, day, day_type) %>% 
  mutate(day = ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  mutate(minutes = as.numeric(minutes))
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The accel\_df contains 50400 observations and 6 columns. Each row
represents a day. From column 4 to column 1443, they each represents a
one-minute interval of a day, so there are 1440 such columns. The
elements in each column represent the activity counts of that minute of
a 63-year-old male with BMI 25 wearing the accelerometer. The day\_type
variable is created for specifying whether that particular day is a
weekday or a weekend.

``` r
total_activity_table = accel_df %>% 
  group_by(day_id, day) %>% 
  summarize(
    total_activity = sum(activity, na.rm = TRUE)
    ) %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'day_id'. You can override using the `.groups` argument.

``` r
total_activity_table
```

| day\_id | day       | total\_activity |
|--------:|:----------|----------------:|
|       1 | Friday    |       480542.62 |
|       2 | Monday    |        78828.07 |
|       3 | Saturday  |       376254.00 |
|       4 | Sunday    |       631105.00 |
|       5 | Thursday  |       355923.64 |
|       6 | Tuesday   |       307094.24 |
|       7 | Wednesday |       340115.01 |
|       8 | Friday    |       568839.00 |
|       9 | Monday    |       295431.00 |
|      10 | Saturday  |       607175.00 |
|      11 | Sunday    |       422018.00 |
|      12 | Thursday  |       474048.00 |
|      13 | Tuesday   |       423245.00 |
|      14 | Wednesday |       440962.00 |
|      15 | Friday    |       467420.00 |
|      16 | Monday    |       685910.00 |
|      17 | Saturday  |       382928.00 |
|      18 | Sunday    |       467052.00 |
|      19 | Thursday  |       371230.00 |
|      20 | Tuesday   |       381507.00 |
|      21 | Wednesday |       468869.00 |
|      22 | Friday    |       154049.00 |
|      23 | Monday    |       409450.00 |
|      24 | Saturday  |         1440.00 |
|      25 | Sunday    |       260617.00 |
|      26 | Thursday  |       340291.00 |
|      27 | Tuesday   |       319568.00 |
|      28 | Wednesday |       434460.00 |
|      29 | Friday    |       620860.00 |
|      30 | Monday    |       389080.00 |
|      31 | Saturday  |         1440.00 |
|      32 | Sunday    |       138421.00 |
|      33 | Thursday  |       549658.00 |
|      34 | Tuesday   |       367824.00 |
|      35 | Wednesday |       445366.00 |

Based on the table, we can see that on weekends(Saturday and Sunday),
the 63-year-old male tended to have lower activity counts in the 35-day
record. Typically, there were two Saturdays which the male seemed to
have no activity at all, or maybe he had taken off the accelerometer on
those days. And on Wednesday, the male normally had activity counts
around 350000 to 450000.

Next, we plot the 24-hour activity time courses for each day.

``` r
  accel_df %>% 
  ggplot(aes(x = minutes, y = activity, group = day_id, color = day)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("0:00", "4:00", "8:00", "12:00", "16:00", "20:00", "24:00")
  )
```

<img src="p8105_hw3_pz2281_files/figure-gfm/unnamed-chunk-16-1.png" width="95%" />

Based on the plot, we can observe that the largest activity counts
happened on Wednesday, around 8pm, and the second was on Sunday, around
12pm. Also, most activities were happened during night time(8pm - 10pm)
as we can see a denser cluster around that interval. The noon was the
second most frequent activity time for the 63-year-old male.
