---
title: "p8105_hw3_pz2281"
author: "Peilin Zhou"
output: github_document
---

```{r}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = .8,
  out.width = "95%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
read in instacart data

```{r}
library(p8105.datasets)
data("instacart")
instacart
```

The instacart dataset contains `r nrow(instacart)` observations with `r ncol(instacart)` variables. Each row represents a product from an order. The data contains customer information with unique user_id as a variable and there are `r length(unique(instacart$user_id))` unique customers. There are total of `r length(unique(instacart$department))` departments, `r length(unique(instacart$aisle))` aisles, and `r length(unique(instacart$product_name))` products in total. The products from each order_id are ordered by the add_to_cart_order from the customer. The order_hour_of_day variable documents the hour of when the order was made. The average order hour of a day for all customers in this dataset is  `r mean(instacart$order_hour_of_day)`.

```{r}
aisle_freq = instacart %>% 
  group_by(aisle) %>% 
  summarize(
    n_obs = n()
  )
most_ord_aisle = aisle_freq %>% 
  slice_max(n_obs)
```

There are `r length(unique(instacart$aisle))` aisles and the aisle that most items are ordered from is `r most_ord_aisle` items, which makes sense in real life, since people often order vegetables online from instacart, amazon fresh etc.

```{r}
aisle_freq %>% 
  filter(n_obs > 10000) %>% 
  arrange(n_obs) %>% 
  mutate(aisle = forcats::fct_inorder(aisle)) %>% 
  ggplot(aes(x = aisle, y = n_obs)) +
  geom_col(alpha = 0.8) +
  labs(
    title = "Number of Items Ordered from Each Aisle",
    x = "Aisle Names",
    y = "Number of Items Ordered"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

From this barchart, we could see that fresh fruits and fresh vegetables are the two aisles that the customers order most from. For each of these two aisles, the number of ordered products are around 150000, which exceed the number of ordered items from other aisles significantly. In contrast, the least ordered aisle among the aisles with ordered items over 10000 is butter.

```{r}
bake_ingr_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "baking ingredients") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)
  
dog_food_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "dog food care") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)

pack_vegfru_order = instacart %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  mutate(order_rank = min_rank(desc(n))) %>% 
  filter(min_rank(desc(n)) < 4) %>% 
  arrange(order_rank)
```


```{r}
order_table = bind_rows(bake_ingr_order, dog_food_order, pack_vegfru_order) %>%   relocate(order_rank, .before = n)
order_table
```

The top 3 products in the baking ingredients aisle are Light Brown Sugar, Pure Baking Soda, and Cane Sugar. It makes sense since these three items are ingredients generally necessary for making breads, cakes, and other desserts. The top 3 products in the dog food care aisle are Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken & Brown Rice Recipe, and Small Dog Biscuits. This shows that people often like to purchase snacks for their dogs online. The top 3 products in the packaged vegetables fruits aisle are Organic Baby Spinach, Organic Raspberries, and Organic Blueberries. It shows that people prefer organic products for fruits and veggies when they purchase packaged ones.

Now move to get tables for pink lady apple and coffee ice cream

```{r}
apple_mean_hour = instacart %>% 
  filter(product_name == "Pink Lady Apples") %>% 
  group_by(order_dow) %>% 
  summarize(
    mean_hour_day = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_day
  ) %>% 
  mutate(item_name = "Pink Lady Apple") %>% 
  relocate(item_name)

cof_ice_mean_hour = instacart %>% 
  filter(product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow) %>% 
  summarize(
    mean_hour_day = mean(order_hour_of_day)
  ) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_day
  ) %>% 
  mutate(item_name = "Coffee Ice Cream") %>% 
  relocate(item_name)

table_mean_hour_day = bind_rows(apple_mean_hour, cof_ice_mean_hour) %>% 
  knitr::kable("simple")
table_mean_hour_day
```

The table shows that customers generally order Pink Lady Apple around noon, from 11am to 13pm. The hours that they order coffee ice cream are generally later, from noon to the afternoon. The average hour of order for each day ranges from 12pm to almost 14pm.


## Problem 2
load the brfss data first

```{r}
data("brfss_smart2010")
```

```{r}
brfss_health_clean = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(state = locationabbr, location_state = locationdesc, resp_id = respid) %>% 
  mutate(resp_id = gsub("RESP", "", resp_id)) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, c("Poor", "Fair", "Good", "Very good", "Excellent")))
#since the responses for the "overall health" question were only in form of poor, fair, good, very good, to excellent, we don't need to do another filter operation.
```

Find states that were observed at 7 or more locations for 2002 and 2010.

```{r}
brfss_health_clean = brfss_health_clean %>% 
  mutate(year = as.character(year))

brfss_health_clean %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  summarise(
    n_loc = n_distinct(location_state)
  ) %>% 
  filter(n_loc >= 7)
```

There were 6 states observed in more than 7 locations for year 2002. They were Connecticut, Florida, Massachusetts, North Carolina, New Jersey, and Pennsylvania.

```{r}
brfss_health_clean %>% 
  filter(year == "2010") %>% 
  group_by(state) %>% 
  summarise(
    n_loc = n_distinct(location_state)
  ) %>% 
  filter(n_loc >= 7)
```

There were 14 states in 2010 which were observed in more than 7 locations. It was 8 states more than that in 2002. The difference shows that more health data were collected in different locations for health assessment.


```{r}
brfss_health_clean %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(
    mean_data_value = mean(data_value)
  ) %>% 
  ggplot(aes(x = year, y = mean_data_value)) +
  geom_line(aes(group = state, color = state)) +
  labs(
    title = "Spaghetti Plot for Average Data Value of Each State across Years"
  )
```

The plot illustrates the average_data_value of the survey participants whose answer were "excellent" to the question about the topic of overall health for each state in a time range from 2002 to 2010. It is clear to observe that in 2005, there is a state that has the lowest mean data value, and the following second lowest were from another state in the year of 2007, and the third lowest was in year 2009, from the same state that has the lowest mean-data-value. The highest mean-data-value was from the year of 2002. 

Next, making plots for the distribution of data_value in NY state from 2006 and 2010.

```{r}
plot_2010 = brfss_health_clean %>% 
  filter(year == "2010" & state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, group = location_state, color = location_state)) +
  geom_line() +
  theme_minimal() + theme(legend.position  = "right") +
  labs(
    title = "Data Values Based on Response Level for Locations in NY state for 2010"
  )

plot_2006 = brfss_health_clean %>% 
  filter(year == "2006" & state == "NY") %>% 
  ggplot(aes(x = response, y = data_value, color = location_state)) +
  geom_line(aes(group = location_state)) +
  labs(
    title = "Data Values Based on Response Level for Locations in NY state for 2006"
  ) +
  theme_minimal() +
  theme(legend.position  = "right")

plot_2006 / plot_2010
```

Based on the two-panel plot, we can see that the data value trend of responses are roughly the same in the year of 2006 and 2010 in NY state. Response as "Good" and "Vary good" have the largest data value in both years. and the data value of response "poor" have the lowest value. The biggest difference between the year of 2006 and 2010 is that, in 2010, response "Very good" has the largest data value, while back in 2006, response "Good" has the largest value.



##Problem 3
load csv data

```{r}
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(day_type = ifelse(day %in% c("Saturday", "Sunday"), "weekend", "weekday")) %>% 
  mutate(day_type = factor(day_type)) %>% 
  relocate(week, day_id, day, day_type) %>% 
  mutate(day = ordered(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minutes",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>% 
  mutate(minutes = as.numeric(minutes))
```

The accel_df contains `r nrow(accel_df)` observations and `r ncol(accel_df)` columns. Each row represents a day. From column 4 to column 1443, they each represents a one-minute interval of a day, so there are 1440 such columns. The elements in each column represent the activity counts of that minute of a 63-year-old male with BMI 25 wearing the accelerometer. The day_type variable is created for specifying whether that particular day is a weekday or a weekend.

```{r}
total_activity_table = accel_df %>% 
  group_by(day_id, day) %>% 
  summarize(
    total_activity = sum(activity, na.rm = TRUE)
    ) %>% 
  knitr::kable()
total_activity_table
```

Based on the table, we can see that on weekends(Saturday and Sunday), the 63-year-old male tended to have lower activity counts in the 35-day record. Typically, there were two Saturdays which the male seemed to have no activity at all, or maybe he had taken off the accelerometer on those days. And on Wednesday, the male normally had activity counts around 350000 to 450000.

Next, we plot the 24-hour activity time courses for each day.
```{r}
  accel_df %>% 
  ggplot(aes(x = minutes, y = activity, group = day_id, color = day)) +
  geom_line(alpha = 0.5) +
  scale_x_continuous(
    breaks = c(0, 240, 480, 720, 960, 1200, 1440),
    labels = c("0:00", "4:00", "8:00", "12:00", "16:00", "20:00", "24:00")
  )
```

Based on the plot, we can observe that the largest activity counts happened on Wednesday, around 8pm,  and the second was on Sunday, around 12pm. Also, most activities were happened during night time(8pm - 10pm) as we can see a denser cluster around that interval. The noon was the second most frequent activity time for the 63-year-old male.












